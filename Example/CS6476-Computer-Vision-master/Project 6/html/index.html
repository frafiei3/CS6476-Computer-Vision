<html>
<head>
<title>Deep Learning Project</title>
<link href='http://fonts.googleapis.com/css?family=Nunito:300|Crimson+Text|Droid+Sans+Mono' rel='stylesheet' type='text/css'>
<link rel="stylesheet" title="Default" href="styles/github.css">
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.3.2/jquery.min.js"></script>  

<link rel="stylesheet" href="highlighting/styles/default.css">
<script src="highlighting/highlight.pack.js"></script>

<style type="text/css">
body {
	margin: 0px;
	width: 100%;
	font-family: 'Crimson Text', serif;
	font-size: 20px;
	background: #fcfcfc;
}
h1 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 28px;
	margin: 25px 0px 0px 0px;
	text-transform: lowercase;

}

h2 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 32px;
	margin: 15px 0px 35px 0px;
	color: #333;	
	word-spacing: 3px;
}

h3 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 26px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}
h4 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 22px;
	margin: 10px 0px 10px 0px;
	color: #333;
	word-spacing: 2px;
}

h5 {
	font-family: 'Nunito', sans-serif;
	font-weight: normal;
	font-size: 18px;
	margin: 10px 0px 10px 0px;
	color: #111;
	word-spacing: 2px;
}

p, li {
	color: #444;
}

a {
	color: #DE3737;
}

.container {
	margin: 0px auto 0px auto;
	width: 1160px;
}

#header {
	background: #333;
	width: 100%;
}

#headersub {
	color: #ccc;
	width: 960px;
	margin: 0px auto 0px auto;
	padding: 20px 0px 20px 0px;
}

.chart {
	width: 480px;
}
.lol {
	font-size: 16px;
	color: #888;
	font-style: italic;
}
.sep {
	height: 1px;
	width: 100%;
	background: #999;
	margin: 20px 0px 20px 0px;
}
.footer{
	font-size: 16px;
}
.latex {
	width: 100%;
}

.latex img {
	display: block;
	margin: 0px auto 0px auto;
}

pre {
	font-family: 'Droid Sans Mono';
	font-size: 14px;
}

table td {
  text-align: center;
  vertical-align: middle;
}

table td img {
  text-align: center;
  vertical-align: middle;
}

#contents a {
}
</style>
<script type="text/javascript">
    hljs.initHighlightingOnLoad();
</script>
</head>
<body>
<div id="header" >
<div id="headersub">
<h1>Yujia Liu <span style="color: #DE3737">(903070716)</span></h1>
</div>
</div>
<div class="container">

<h2> Project 6 / Deep Learning</h2>

<p>In this project, I design and train deep convolutional networks for scene recognition using the MatConvNet toolbox. This project is mainly divided into two parts. First, I train a deep convolutional network from scratch to recognize scenes. Given network performs poorly and some modifications and adjustions are needed in order to enhance accuracy. Then, for part 2, I fine-tune a pre-trained deep network to achieve better performance on this task. The pre-train network I'm using is VGG-F network. This network is not designed to classify scenes, however, with minor tuning, the performance is still exceeds the hand-crafted feature I implemented for the last project.</p>

<div style="clear:both">

<h3>Part 1</h3>

<p>The network given by the stencil code is rather simple and can achieve only 25% accuracy by itself. According to the instruction, besides the given network, I put in additional one convolutional layer, followed by a max pool layer, followed by a rectified linear layer. The weight matrix, i.e. parameters, is randomly assigned at first, and is learned through the training process of the network. After configuration and adjustion, the accuracy can be improved to 50% approximately. The network structure is designed as shown below.</p>

<pre><code>
net.layers = {} ;

net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'conv1') ;
                       
net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;

net.layers{end+1} = struct('type', 'relu') ;

net.layers{end+1} = struct('type', 'dropout', 'rate', 0.5) ;

net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(9,9,1,10, 'single'), zeros(1, 10, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'conv1') ;

net.layers{end+1} = struct('type', 'pool', ...
                           'method', 'max', ...
                           'pool', [2 2], ...
                           'stride', 2, ...
                           'pad', 0) ;
                    
net.layers{end+1} = struct('type', 'relu') ;

net.layers{end+1} = struct('type', 'dropout', 'rate', 0.5) ;

net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(6,6,10,15, 'single'), zeros(1, 15, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'fc1') ;     
                    
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(5,5,15,15, 'single'), zeros(1, 15, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'fc1') ;                        
                   
% Loss layer
net.layers{end+1} = struct('type', 'softmaxloss') ;
</code></pre>

<p>Some modifications according to the instruction are listed below, folowed by the performance plot.</p>

<p>Adding more training examples by fliping some fraction of the images, 32.6% accuracy is achieved.</p>

<pre><code>
flip_ind = randi(50,1,15);
im(:,:,1,flip_ind) = fliplr(im(:,:,1,flip_ind));
</code></pre>

<center>
<img src="jitter.bmp">
</center>

<p>Zero-center images by subtracting the mean of images, 48.0% accuracy is achieved</p>

<pre><code>
image_mean = mean2(cur_image);
cur_image = cur_image - image_mean;
</code></pre>

<center>
<img src="zero-center.bmp">
</center>

<p>Regularize the network by adding a dropout layer, 54.7% accuracy is achieved</p>

<pre><code>
net.layers{end+1} = struct('type', 'dropout', 'rate', 0.5);
</code></pre>

<center>
<img src="regularize.bmp">
</center>

<p>All the modifications above attribute to the performance improvement by some amounts and the overall accuracy obtained is 51.5%. The additional layers are not helpful due to the lack of tuning. The result plot and the filters in the first layer is shown below.</p>

<center>
<img src="deep.bmp">
<img src="deep2.bmp">
</center>

<h3>Part2</h3>

<p>Despite the modifications we did above, the performance is still not as well as we did for the last project by manually creating the features, such as gist feature. One solution is to take advantage of some pre-train networks to achieve better performance. The key idea here is that the representations learned by the pre-train network generalize surprisingly well to other recognition tasks. For this task, given the VGG-F network, I replace the last two layers with my own layers to adapt to the dimension of input required, add two dropout layers. The resulting network structure modifications are shown below.</p>

<pre><code>
net.layers{20} = net.layers{19};
net.layers{19} = net.layers{18};
net.layers{18} = struct('type', 'dropout', 'rate', 0.5) ;
net.layers{21} = struct('type', 'dropout', 'rate', 0.5) ;
net.layers{end+1} = struct('type', 'conv', ...
                           'weights', {{f*randn(1,1,4096,15, 'single'), zeros(1, 15, 'single')}}, ...
                           'stride', 1, ...
                           'pad', 0, ...
                           'name', 'fc8') ;                                           
net.layers{end+1} = struct('type', 'softmaxloss') ;
</code></pre>

<p>The performance of this network is very impressive. In five epoch, it achieves over 85% accuracy. The result plot and the filters inthe first layer are shown below.</p>

<center>
<img src="part2.bmp">
<img src="part2_2.bmp">
</center>

</body>
</html>